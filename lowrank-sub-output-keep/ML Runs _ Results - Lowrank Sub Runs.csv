W&B Name,Seeds,Label,base model,Training,Init,Total DS,Test split,num layers,hidden size (embedding size),sa ranks,mlp ranks,Trainable params,params delta from baseline,Batch Size,Grad Accum Steps,Effective Batch Size,Training args,VRAM max (MB),eval loss @ 6k,eval loss delta from baseline,It / sec,Training time (min),Output,Notes
lrs-run-1,"0,42",FullSA-FullMLP-4L,GPTNeo,from scratch,random,25k,0.1,4,384,None,None,"34,266,240",0.000,4,1,4,"    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    evaluation_strategy=""steps"",
    logging_strategy=""steps"",
    save_steps=500,
    eval_steps=1000,
    logging_steps=500,",5996,3.275,0.000,2.84,36.3,"TrainOutput(global_step=6188, training_loss=3.7962034949196024, metrics={'train_runtime': 2175.816, 'train_samples_per_second': 11.375, 'train_steps_per_second': 2.844, 'total_flos': 2101682995799040.0, 'train_loss': 3.7962034949196024, 'epoch': 1.0})","Baseline. BTW, attention types are GPTNeo default: [[[""global"", ""local""]], 2]. Should be exactly the same as lrs-run-0, but I've made some code changes so want to validate."
lrs-run-4,"0,42",LgSA-LgMLP-4L,GPTNeo,from scratch,random,25k,0.1,4,384,"[192, 192, 192, 192]","[307, 307, 307, 307]","34,263,168",0.000,4,1,4,"    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    evaluation_strategy=""steps"",
    logging_strategy=""steps"",
    save_steps=500,
    eval_steps=1000,
    logging_steps=500,",6006,3.377,0.031,2.79,37.0,"TrainOutput(global_step=6188, training_loss=3.864639134274503, metrics={'train_runtime': 2220.3034, 'train_samples_per_second': 11.147, 'train_steps_per_second': 2.787, 'total_flos': 2101227716514816.0, 'train_loss': 3.864639134274503, 'epoch': 1.0})",Re-running lrs-run-2 but with the correct math. So expected to be roughly the same as lrs-run-1 but with the LoRA machinery in place.
lrs-run-5,"0,42",MedSA-FullMLP-4L,GPTNeo,from scratch,random,25k,0.1,4,384,"[19, 19, 19, 19]",None,"32,140,416",-0.062,4,1,4,"    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    evaluation_strategy=""steps"",
    logging_strategy=""steps"",
    save_steps=500,
    eval_steps=1000,
    logging_steps=500,",5960,3.562,0.088,2.89,35.7,"TrainOutput(global_step=6188, training_loss=3.961234420056334, metrics={'train_runtime': 2140.6693, 'train_samples_per_second': 11.562, 'train_steps_per_second': 2.891, 'total_flos': 1786629731116032.0, 'train_loss': 3.961234420056334, 'epoch': 1.0})","Moderate rank but on the SA only.

Pretty big eval hit (8.9%) for not much savings in parameters (6.2%)"
lrs-run-6,"0,42",FullSA-MedMLP-4L,GPTNeo,from scratch,random,25k,0.1,4,384,None,"[31, 31, 31, 31]","30,023,808",-0.124,4,1,4,"    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    evaluation_strategy=""steps"",
    logging_strategy=""steps"",
    save_steps=500,
    eval_steps=1000,
    logging_steps=500,",5960,3.567,0.089,2.98,34.6,"TrainOutput(global_step=6188, training_loss=4.047985291434783, metrics={'train_runtime': 2074.7332, 'train_samples_per_second': 11.929, 'train_steps_per_second': 2.983, 'total_flos': 1472942304285696.0, 'train_loss': 4.047985291434783, 'epoch': 1.0})","Moderate rank but on the MLP only.

Almost exactly the same as SA moderate rank (lrs-run-5)."
lrs-run-7,"0,42",MedSA-MedMLP-4L,GPTNeo,from scratch,random,25k,0.1,4,384,"[19, 19, 19, 19]","[31, 31, 31, 31]","27,897,984",-0.186,4,1,4,"    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    evaluation_strategy=""steps"",
    logging_strategy=""steps"",
    save_steps=500,
    eval_steps=1000,
    logging_steps=500,",5930,3.939,0.203,3.03,34.0,"TrainOutput(global_step=6188, training_loss=4.364889133801056, metrics={'train_runtime': 2042.247, 'train_samples_per_second': 12.119, 'train_steps_per_second': 3.03, 'total_flos': 1157889039602688.0, 'train_loss': 4.364889133801056, 'epoch': 1.0})","Moderate rank on both SA and MLP.

I'm at a loss why the #params seems so poorly correlated with the VRAM consumption. This model has 81% of the params of the baseline, but consumes 99% of the VRAM!"
lrs-run-8,"0,42",MedSA-MedMLP-8L,GPTNeo,from scratch,random,25k,0.1,8,384,"[19, 19, 19, 19, 19, 19, 19, 19]","[31, 31, 31, 31, 31, 31, 31, 31]","35,710,080",0.042,4,1,4,"    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    evaluation_strategy=""steps"",
    logging_strategy=""steps"",
    save_steps=500,
    eval_steps=1000,
    logging_steps=500,",7544,3.884,0.186,2.01,51.4,"TrainOutput(global_step=6188, training_loss=4.297692681715114, metrics={'train_runtime': 3082.154, 'train_samples_per_second': 8.03, 'train_steps_per_second': 2.008, 'total_flos': 2315664259384320.0, 'train_loss': 4.297692681715114, 'epoch': 1.0})","Stick with moderate rank on SA and MLP (like lrs-run-7) and use the savings in params to go from 4 to 8 layers

Quite disappointing!"
lrs-run-9,"0,42",IncSA-FullMLP-4L,GPTNeo,from scratch,random,25k,0.1,4,384,"[5, 20, 50, 100]",None,"32,444,544",-0.053,4,1,4,"    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    evaluation_strategy=""steps"",
    logging_strategy=""steps"",
    save_steps=500,
    eval_steps=1000,
    logging_steps=500,",5964,3.505,0.070,2.89,35.7,"TrainOutput(global_step=6188, training_loss=3.930144652136696, metrics={'train_runtime': 2144.1848, 'train_samples_per_second': 11.543, 'train_steps_per_second': 2.886, 'total_flos': 1831702380254208.0, 'train_loss': 3.930144652136696, 'epoch': 1.0})","4 layers. SA rank low to high, MLP rank default"
lrs-run-10,"0,42",DecSA-FullMLP-4L,GPTNeo,from scratch,random,25k,0.1,4,384,"[100, 50, 20, 5]",None,"32,444,544",-0.053,4,1,4,"    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    evaluation_strategy=""steps"",
    logging_strategy=""steps"",
    save_steps=500,
    eval_steps=1000,
    logging_steps=500,",5964,3.468,0.059,2.88,35.8,"TrainOutput(global_step=6188, training_loss=3.9026858348883424, metrics={'train_runtime': 2149.4061, 'train_samples_per_second': 11.515, 'train_steps_per_second': 2.879, 'total_flos': 1831702380254208.0, 'train_loss': 3.9026858348883424, 'epoch': 1.0})","4 layers. SA rank high to low, MLP rank default"
lrs-run-11,"0,42",FullSA-IncMLP-4L,GPTNeo,from scratch,random,25k,0.1,4,384,None,"[10, 30, 80, 150]","30,584,448",-0.107,4,1,4,"    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    evaluation_strategy=""steps"",
    logging_strategy=""steps"",
    save_steps=500,
    eval_steps=1000,
    logging_steps=500,",5970,3.563,0.088,2.96,34.9,"TrainOutput(global_step=6188, training_loss=4.051314508676066, metrics={'train_runtime': 2090.7418, 'train_samples_per_second': 11.838, 'train_steps_per_second': 2.96, 'total_flos': 1556030773656576.0, 'train_loss': 4.051314508676066, 'epoch': 1.0})","4 layers. SA rank default, MLP rank low to high"
lrs-run-12,"0,42",FullSA-DecMLP-4L,GPTNeo,from scratch,random,25k,0.1,4,384,None,"[150, 80, 30, 10]","30,584,448",-0.107,4,1,4,"    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    evaluation_strategy=""steps"",
    logging_strategy=""steps"",
    save_steps=500,
    eval_steps=1000,
    logging_steps=500,",5970,3.562,0.088,2.96,34.9,"TrainOutput(global_step=6188, training_loss=4.044674735879929, metrics={'train_runtime': 2093.9003, 'train_samples_per_second': 11.82, 'train_steps_per_second': 2.955, 'total_flos': 1556030773656576.0, 'train_loss': 4.044674735879929, 'epoch': 1.0})","4 layers. SA rank default, MLP rank high to low"
lrs-run-13,"1,1",FullSA-FullMLP-4L,GPTNeo,from scratch,random,25k,0.1,4,384,None,None,"34,266,240",0.000,4,1,4,"    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    evaluation_strategy=""steps"",
    logging_strategy=""steps"",
    save_steps=500,
    eval_steps=1000,
    logging_steps=500,",,3.216,-0.018,,,,
lrs-run-14,"2,2",FullSA-FullMLP-4L,GPTNeo,from scratch,random,25k,0.1,4,384,None,None,"34,266,240",0.000,4,1,4,"    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    evaluation_strategy=""steps"",
    logging_strategy=""steps"",
    save_steps=500,
    eval_steps=1000,
    logging_steps=500,",,3.204,-0.022,,,,
lrs-run-15,"3,3",FullSA-FullMLP-4L,GPTNeo,from scratch,random,25k,0.1,4,384,None,None,"34,266,240",0.000,4,1,4,"    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    evaluation_strategy=""steps"",
    logging_strategy=""steps"",
    save_steps=500,
    eval_steps=1000,
    logging_steps=500,",,3.252,-0.007,,,,
lrs-run-16,"4,4",FullSA-FullMLP-4L,GPTNeo,from scratch,random,25k,0.1,4,384,None,None,"34,266,240",0.000,4,1,4,"    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    evaluation_strategy=""steps"",
    logging_strategy=""steps"",
    save_steps=500,
    eval_steps=1000,
    logging_steps=500,",,3.294,0.006,,,,
